#!/usr/bin/pythonfrom __future__ import divisionimport numpy as npfrom scipy.optimize import minimizeimport itertoolsimport numpy.linalg as LAfrom operator import itemgetterimport timefrom sklearn.feature_extraction.text import CountVectorizerimport pandas as pdfrom scipy.sparse import csr_matrixfrom scipy.sparse import coo_matriximport operatorimport itertoolsimport nltkfrom nltk.collocations import *import argparseimport jsonimport os# this script is the implementation of DQE using relevance score# for more information please read the paper "Cyberbullying Detection with Weakly Supervised Machine Learning"# http://people.cs.vt.edu/~bhuang/papers/raisi-asonam17.pdfsave_dir = './data/synthetic/saved_data/PVC/'if not os.path.exists(save_dir):    os.makedirs(save_dir)# cleaned data set. All of the preprocessing steps have been done in cleaned data. The data is saved in json formatdataset = 'synthetic.txt'# This function computes the number of extracted documents containing seed words (fouls)def get_number_doc(all_tweets, fouls):    numb_doc = 0    DF = dict()    for idx, words in all_tweets.items():        if len(fouls.intersection(words)) > 0:            numb_doc += 1            for word in words:                if word in DF:                    DF[word] += 1                else:                    DF[word] = 1    return numb_doc, DFdef main():    Main_badwords = []    with open('./data/' + args.data_set + '/files/'+'badwords.txt') as f:        for key in f:            Main_badwords.append(key[1:-2])    fouls = []    with open('./data/synthetic/files/badwords.txt') as f:        for key in f:            if key:                fouls.append(key.strip())    # load bigrams    all_bigrams = set()    with open('./data/synthetic/files/bigrams.txt') as f:        for key in f:            all_bigrams.add(key.strip())    all_tweets = {}    num_doc = 0    with open('./data/synthetic/files/' +  dataset) as json_file:        data = json.load(json_file)        for t in data['tweet']:            wd = []            value = t['words']            for w in value.split():                if w.strip():                    wd.append(w)            words = list(set(wd))            if t['in_reply_to_user_id'] != 'None' and t['in_reply_to_user_id'] != t['User_id'] and words != []:                wordSet = set(t['words'].split())                num_doc += 1                finder = BigramCollocationFinder.from_words(t['words'].split())                bigrams = (finder.ngram_fd.items())                new_bigrams = [' '.join(map(str, pp[0])) for pp in bigrams]                my_bi = set()                for bi in new_bigrams:                    if bi in all_bigrams:                        wordSet.add(bi)                        if bi not in fouls:                            my_bi.add(bi)                all_tweets[num_doc] = wordSet    new_fouls = set()    new_fouls_dic = {}    for iteration in range(4):        dic = {}        print("Starting iteration %d" % iteration)        num_doc, DF = get_number_doc(all_tweets, fouls)        for idx,words in all_tweets.items():            if len(fouls.intersection(words)) > 0:                for w in words.difference(fouls):                    dic[w] = DF[w] / num_doc        sorted_x = sorted(dic.items(), key=operator.itemgetter(1),reverse = True)        candidates =  sorted_x[:4000]        for word, score in candidates:            if word not in fouls:                new_fouls.add(word)                new_fouls_dic[word] = score                fouls.add(word)                swear_words[word] = score        print("%d words in expanded query at iteration %d" % (len(new_fouls), iteration))    f = open(save_dir + '/DQE.txt', 'w')    for key,value in sorted(new_fouls_dic.items(), key=operator.itemgetter(1), reverse = True):        f.write(str(key[0]) + ',' + str(key[1]))        f.write('\n')if __name__ == "__main__":    main()